{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JnmdJsMZK55"
   },
   "source": [
    "# Transformers Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook is showed the training of 3 BERT's family moddels: DistilBERT, BERT-base, RoBERTa-base. The goal was to evaluate these models in order to choose the one with best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ATTENTION!** The training arguments are set such that each trained model is saved in a directory called as *model_name*!!!\n",
    "\n",
    "In order to do **NOT SAVE** them, **check the TrainingArgs parameters** in each section of examined models, where are initialized. **Check** those sections to **avoid** undesired save strategy for log and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104490,
     "status": "ok",
     "timestamp": 1746176443483,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "8LM1iBt8KPHc",
    "outputId": "ff8a92a4-1d75-48ef-c03d-5a64179680d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: wikidata in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: transformers in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (2.0.1)\n",
      "Requirement already satisfied: dill in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.7.4.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate wikidata transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 35414,
     "status": "ok",
     "timestamp": 1746176478911,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "3lRNucX-ZK58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import evaluate\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "from wikidata.client import Client\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, Trainer, TrainingArguments, DataCollatorWithPadding, set_seed, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746176478918,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "n366eQk50yvX"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMFmU7zwpOff"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1746176478960,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "f8IDPKdrZK5-"
   },
   "outputs": [],
   "source": [
    "dir_path = \"/mnt/c/Users/fede6/Desktop/HW1/\"\n",
    "train_path = \"train.csv\"\n",
    "dev_path = \"valid.csv\"\n",
    "test_path =  \"test_unlabeled.csv\"\n",
    "\n",
    "train_df = pd.read_csv(dir_path + train_path, encoding='utf-8')\n",
    "dev_df = pd.read_csv(dir_path + dev_path, encoding='utf-8')\n",
    "test_df = pd.read_csv(dir_path + test_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1746176498198,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sfBUCdZNZK5_"
   },
   "outputs": [],
   "source": [
    "def save_txt(filename, path, txt):\n",
    "    with open(path + filename, 'w', encoding='utf-8') as output:\n",
    "        json.dump(txt, output, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_txt(filename, path):\n",
    "    with open(path + filename, 'r', encoding='utf-8') as input_file:\n",
    "        return json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746176478985,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "VGYXlXkOJ_Ap"
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "def extract_qid(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def get_wiki_link(qid, lang='en'):\n",
    "    try:\n",
    "        entity = client.get(qid, load=True)\n",
    "        sitelinks = entity.data.get('sitelinks', {})\n",
    "        page_info = sitelinks.get(f'{lang}wiki')\n",
    "        return page_info['url'] if page_info else None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR retrieving Wikipedia link for {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_paragraphs(wikipedia_link):\n",
    "    try:\n",
    "        response = requests.get(wikipedia_link, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.find('div', class_='mw-content-ltr mw-parser-output')\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = []\n",
    "        for p in content.find_all('p'):\n",
    "            text = p.get_text(separator=\" \", strip=True)\n",
    "            text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR! Link {wikipedia_link}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_item(index, item, df, lang):\n",
    "    try:\n",
    "        qid = extract_qid(item)\n",
    "        link = get_wiki_link(qid, lang)\n",
    "        paragraph = get_paragraphs(link)\n",
    "\n",
    "        if not link:\n",
    "            print(f\"WARNING: missing Wikipedia link for QID {qid}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        if not paragraph:\n",
    "            print(f\"WARNING: empty or missing content for {link}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        return index, paragraph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing item {item} (QID: {qid if 'qid' in locals() else 'UNKNOWN'}): {e}\")\n",
    "        return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "def text_extraction(df, lang='en', max_workers=16):\n",
    "    results = [None] * len(df)\n",
    "    items = list(enumerate(df['item']))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_item, idx, item, df, lang): idx for idx, item in items}\n",
    "\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            idx, paragraph = future.result()\n",
    "            results[idx] = paragraph\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746176478992,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "PTeBItU0J_Ar"
   },
   "outputs": [],
   "source": [
    "EXTRACTED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1746176502640,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "aWBRU1d1J_As"
   },
   "outputs": [],
   "source": [
    "if EXTRACTED:\n",
    "    train_txt = load_txt(filename=\"train_txts.txt\", path=dir_path)\n",
    "    valid_txt = load_txt(filename=\"dev_txts.txt\",   path=dir_path)\n",
    "    test_txt = load_txt(filename=\"test_txts.txt\",   path=dir_path)\n",
    "else:\n",
    "    train_txt = text_extraction(df=train_df)\n",
    "    valid_txt = text_extraction(df=dev_df)\n",
    "    test_txt  = text_extraction(df=test_df)\n",
    "\n",
    "train_df['paragraph'] = train_txt\n",
    "dev_df['paragraph'] = valid_txt\n",
    "test_df['paragraph'] = test_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1746176506896,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "Jhrw8OZVJ_At",
    "outputId": "562d9628-c35a-4a83-e980-d28398f862b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q32786</td>\n",
       "      <td>916</td>\n",
       "      <td>2012 film by M. Mohanan</td>\n",
       "      <td>entity</td>\n",
       "      <td>films</td>\n",
       "      <td>film</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>916 is a 2012 Indian Malayalam -language drama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q371</td>\n",
       "      <td>!!!</td>\n",
       "      <td>American dance-punk band from California</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>musical group</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>!!! ( / tʃ ( ɪ ) k . tʃ ( ɪ ) k . tʃ ( ɪ ) k /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3729947</td>\n",
       "      <td>¡Soborno!</td>\n",
       "      <td>Mort &amp; Phil comic</td>\n",
       "      <td>entity</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>comics</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>¡Soborno! (English: Bribery! ) is a 1977 comic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q158611</td>\n",
       "      <td>+44</td>\n",
       "      <td>American band</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>musical group</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>+44 (read as Plus Forty-four ) was an American...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q280375</td>\n",
       "      <td>1 Monk Street</td>\n",
       "      <td>building in Monmouth, Wales</td>\n",
       "      <td>entity</td>\n",
       "      <td>architecture</td>\n",
       "      <td>building</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>1 Monk Street, Monmouth was built as a Working...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      item           name  \\\n",
       "0    http://www.wikidata.org/entity/Q32786            916   \n",
       "1      http://www.wikidata.org/entity/Q371            !!!   \n",
       "2  http://www.wikidata.org/entity/Q3729947      ¡Soborno!   \n",
       "3   http://www.wikidata.org/entity/Q158611            +44   \n",
       "4   http://www.wikidata.org/entity/Q280375  1 Monk Street   \n",
       "\n",
       "                                description    type          category  \\\n",
       "0                   2012 film by M. Mohanan  entity             films   \n",
       "1  American dance-punk band from California  entity             music   \n",
       "2                         Mort & Phil comic  entity  comics and anime   \n",
       "3                             American band  entity             music   \n",
       "4               building in Monmouth, Wales  entity      architecture   \n",
       "\n",
       "     subcategory                    label  \\\n",
       "0           film       cultural exclusive   \n",
       "1  musical group  cultural representative   \n",
       "2         comics  cultural representative   \n",
       "3  musical group  cultural representative   \n",
       "4       building       cultural exclusive   \n",
       "\n",
       "                                           paragraph  \n",
       "0  916 is a 2012 Indian Malayalam -language drama...  \n",
       "1  !!! ( / tʃ ( ɪ ) k . tʃ ( ɪ ) k . tʃ ( ɪ ) k /...  \n",
       "2  ¡Soborno! (English: Bribery! ) is a 1977 comic...  \n",
       "3  +44 (read as Plus Forty-four ) was an American...  \n",
       "4  1 Monk Street, Monmouth was built as a Working...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NARyCuIJ_At"
   },
   "source": [
    "### **Attention**!\n",
    "In order to deliver the predicitons on the unlabeled test_set. In order to use test is sufficient to do set **is_labeled=True**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 108398,
     "status": "aborted",
     "timestamp": 1746176479316,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "nAzfHZZlpOfh"
   },
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    'cultural agnostic':       2,\n",
    "    'cultural representative': 1,\n",
    "    'cultural exclusive':      0\n",
    "}\n",
    "\n",
    "class PLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, df, is_labeled=True):\n",
    "        self.is_labeled = is_labeled\n",
    "        self.encodings = encodings\n",
    "        self.size = len(df['item'].to_list())\n",
    "        if self.is_labeled:\n",
    "            self.labels = [mapper[label] for label in df['label']]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]).to(device) for k, v in self.encodings.items()}\n",
    "\n",
    "        if self.is_labeled:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOK9uFuqqiVt"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746175653921,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "zaAJp8vzqkS_"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_precision = evaluate.load(\"precision\")\n",
    "   load_recall = evaluate.load(\"recall\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   precision = load_precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "   recall = load_recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_9p48HNroW-"
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746175656094,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sNU3_3WLrrF_"
   },
   "outputs": [],
   "source": [
    "def model_init(model_name, n_classes=3, padding=True, truncation=True):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True, output_attentions=False, output_hidden_states=False, num_labels=n_classes).to(device)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "  return model, tokenizer, data_collator\n",
    "\n",
    "def tokenization(df, tokenizer):\n",
    "    return tokenizer(df[\"paragraph\"].to_list(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-64lA6eJ_Aw"
   },
   "source": [
    "#### Training global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746175661322,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "WbWLwTFNJ_Aw"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 391\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFl1YZoypOfi"
   },
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746175663166,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "lL3hansiZK5_"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg1_fGFJpOfj"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71579,
     "status": "ok",
     "timestamp": 1746176184256,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "uiYDuTRNZK6A",
    "outputId": "75ab2686-c24b-4384-bd60-27d23a8224f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, is_labeled=True)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, is_labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746175866499,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "fS2h8HDwZK6B"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 99972,
     "status": "error",
     "timestamp": 1746175976208,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "_pPwmKC6ZK6C",
    "outputId": "6c8296ee-e957-4baf-cabe-e33c5de128df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2737' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2737/3910 17:39 < 07:34, 2.58 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.881600</td>\n",
       "      <td>0.621045</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.705515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.559214</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.744374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.762704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.600148</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.751146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.768821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.750408</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.745849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.816580</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.740811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2737, training_loss=0.4816564212195578, metrics={'train_runtime': 1059.653, 'train_samples_per_second': 29.496, 'train_steps_per_second': 3.69, 'total_flos': 2898570840966144.0, 'train_loss': 0.4816564212195578, 'epoch': 3.5})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 5318,
     "status": "ok",
     "timestamp": 1745858556581,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "bSRgVCQnZK6C",
    "outputId": "d37329ba-e4e5-4bcd-c93d-f3eb9029d3a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.534774661064148,\n",
       " 'eval_accuracy': 0.7766666666666666,\n",
       " 'eval_f1': 0.762704101582325,\n",
       " 'eval_runtime': 5.0213,\n",
       " 'eval_samples_per_second': 59.745,\n",
       " 'eval_steps_per_second': 7.568,\n",
       " 'epoch': 3.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taN8euqVpOfl"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yJNbgDEpOfm"
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-hTTqaY_IIU",
    "outputId": "1f8e20aa-cc8f-411d-cf6a-58aca8a7344c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, is_labeled=True)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, is_labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0m7JGXw_IIV"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NferTjq9pOfm",
    "outputId": "0e5b51bf-ed88-46de-a0ff-5573b0e28333"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/3910 29:04 < 19:23, 1.34 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.873700</td>\n",
       "      <td>0.654126</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.631042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.747070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.615110</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.737141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.740719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.750575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.793203</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.740052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.49736401166858984, metrics={'train_runtime': 1744.8959, 'train_samples_per_second': 17.912, 'train_steps_per_second': 2.241, 'total_flos': 4934165922653184.0, 'train_loss': 0.49736401166858984, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixz2xk-kpOfn",
    "outputId": "c2acbfff-24af-402d-c4cc-7c3560c7ae28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5664123296737671,\n",
       " 'eval_accuracy': 0.7633333333333333,\n",
       " 'eval_f1': 0.747069841162474,\n",
       " 'eval_runtime': 10.7632,\n",
       " 'eval_samples_per_second': 27.873,\n",
       " 'eval_steps_per_second': 3.531,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14gduCy_pOfn"
   },
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJ18vZgBCmPR"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3L3ZRA7pOfn"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTYy3v98Cd7R",
    "outputId": "0ccf11c0-eb2d-4a21-e54a-6a3cd25231a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, is_labeled=True)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, is_labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWC_S5KnCd7R"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AInJCUkpOfo",
    "outputId": "ab546a9e-a6cd-4791-a3cc-cf4cb454b14b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/3910 29:25 < 19:37, 1.33 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.602486</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.721386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.575793</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.794663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.578462</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.768546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.618061</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.776604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.731010</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.771274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.866831</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.740185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.5111339468277121, metrics={'train_runtime': 1766.1796, 'train_samples_per_second': 17.696, 'train_steps_per_second': 2.214, 'total_flos': 4934165922653184.0, 'train_loss': 0.5111339468277121, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRRJtDRhpOfo",
    "outputId": "ab8e4d9c-79ba-43a3-9fcc-b38c4b0a8bfa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5757932066917419,\n",
       " 'eval_accuracy': 0.8033333333333333,\n",
       " 'eval_f1': 0.7946633866399488,\n",
       " 'eval_runtime': 9.9552,\n",
       " 'eval_samples_per_second': 30.135,\n",
       " 'eval_steps_per_second': 3.817,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xVcGCqaJ_A2"
   },
   "source": [
    "After different trials, the RoBERTa-base model showed better performance with respect to the DistilBERT and BERT-base models.  \n",
    "All the models showed that after 1 epoch the train loss and validation loss start to diverge. In order to show this behavior, the models has been trained for at most 5 epochs and the early stopping callback has been used to interrupt the training when the validation loss increase for 4 *eval_steps*.  \n",
    "In the next section [**Best Model**], the best trained RoBERTa-base model predictions are showed and saved as file csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLtO10iwJ_A7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dbBPnSqJ_A7"
   },
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOIFTHP-J_A7"
   },
   "source": [
    "After several experiments, observing the metrics on the development set, the best model is **RoBERTa-base model**. Here is presented the evaluation metrics on the validation set and then the predictions on the first elements of the test set. The best model's predictions are saved in file csv, delivered as final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7NfzvbagJ_A7"
   },
   "outputs": [],
   "source": [
    "best_model_path = dir_path + \"BestRes/results/checkpoint-782\"\n",
    "\n",
    "model, tokenizer, data_coll = model_init(best_model_path)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, is_labeled=True)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, is_labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EvaoVnUxJ_A7"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0FYJTHwBJ_A8",
    "outputId": "c4db3f9f-eda6-43b1-834d-5ce069c34d20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 03:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dw6v7sOKJ_A8",
    "outputId": "ed05e613-999a-477a-fd8b-2a87062e1f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa base score:\n",
      "Accuracy:  0.8100\n",
      "Precision: 0.8030\n",
      "Recall:    0.7989\n",
      "F1 score:  0.8007\n"
     ]
    }
   ],
   "source": [
    "accuracy = results['eval_accuracy']\n",
    "precision = results['eval_precision']\n",
    "recall = results['eval_recall']\n",
    "f1_score = results['eval_f1']\n",
    "\n",
    "print(f\"RoBERTa base score:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 score:  {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TEST PHASE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the labeled test set, set **is_labeled = True** for evaluating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "h2e9_W1uJ_A8"
   },
   "outputs": [],
   "source": [
    "test_dataset = PLMDataset(tokenization(test_df, tokenizer=tokenizer), df=test_df, is_labeled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IS_TEST_LABELED** is set as **is_labeled**, so in order to evaluate the model on the labeled test set, check the **is_labeled** parameter on the PLMDataset initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TEST_LABELED = test_dataset.is_labeled\n",
    "\n",
    "if IS_TEST_LABELED:\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    test_accuracy = test_results['eval_accuracy']\n",
    "    test_precision = test_results['eval_precision']\n",
    "    test_recall = test_results['eval_recall']\n",
    "    test_f1_score = test_results['eval_f1']\n",
    "    \n",
    "    print(f\"RoBERTa base score:\")\n",
    "    print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall:    {test_recall:.4f}\")\n",
    "    print(f\"F1 score:  {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PKkmn-JzJ_A8"
   },
   "outputs": [],
   "source": [
    "preds_struct = trainer.predict(test_dataset=test_dataset)\n",
    "predictions = np.argmax(preds_struct.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKRoWz7fJ_A8"
   },
   "source": [
    "The labels of the predictions have been mapped in the corresponding cultural class in order to provide a easier readable table. This mapping can be easily removed setting **REMAP = False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1WUharDtJ_A9",
    "outputId": "9bc13af2-41ab-4c04-d178-c6df682d0436"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2427430</td>\n",
       "      <td>Northeast Flag Replacement</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q125482</td>\n",
       "      <td>imam</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15789</td>\n",
       "      <td>FC Bayern Munich</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q582496</td>\n",
       "      <td>Fome Zero</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q572811</td>\n",
       "      <td>Anthony Award</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1866547</td>\n",
       "      <td>Livraria Bertrand</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.wikidata.org/entity/Q19081</td>\n",
       "      <td>prokaryotes</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.wikidata.org/entity/Q474090</td>\n",
       "      <td>narrative poetry</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1266300</td>\n",
       "      <td>Neue Slowenische Kunst</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.wikidata.org/entity/Q193654</td>\n",
       "      <td>short-track speed skating</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://www.wikidata.org/entity/Q372</td>\n",
       "      <td>We Live in Public</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://www.wikidata.org/entity/Q820887</td>\n",
       "      <td>University of Florence</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.wikidata.org/entity/Q503269</td>\n",
       "      <td>seamount</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.wikidata.org/entity/Q57318</td>\n",
       "      <td>free imperial city</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://www.wikidata.org/entity/Q25239</td>\n",
       "      <td>Alnus</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       item                        name  \\\n",
       "0   http://www.wikidata.org/entity/Q2427430  Northeast Flag Replacement   \n",
       "1    http://www.wikidata.org/entity/Q125482                        imam   \n",
       "2     http://www.wikidata.org/entity/Q15789            FC Bayern Munich   \n",
       "3    http://www.wikidata.org/entity/Q582496                   Fome Zero   \n",
       "4    http://www.wikidata.org/entity/Q572811               Anthony Award   \n",
       "5   http://www.wikidata.org/entity/Q1866547           Livraria Bertrand   \n",
       "6     http://www.wikidata.org/entity/Q19081                 prokaryotes   \n",
       "7    http://www.wikidata.org/entity/Q474090            narrative poetry   \n",
       "8   http://www.wikidata.org/entity/Q1266300      Neue Slowenische Kunst   \n",
       "9    http://www.wikidata.org/entity/Q193654   short-track speed skating   \n",
       "10      http://www.wikidata.org/entity/Q372           We Live in Public   \n",
       "11   http://www.wikidata.org/entity/Q820887      University of Florence   \n",
       "12   http://www.wikidata.org/entity/Q503269                    seamount   \n",
       "13    http://www.wikidata.org/entity/Q57318          free imperial city   \n",
       "14    http://www.wikidata.org/entity/Q25239                       Alnus   \n",
       "\n",
       "                predictions  \n",
       "0        cultural exclusive  \n",
       "1   cultural representative  \n",
       "2   cultural representative  \n",
       "3        cultural exclusive  \n",
       "4        cultural exclusive  \n",
       "5        cultural exclusive  \n",
       "6         cultural agnostic  \n",
       "7         cultural agnostic  \n",
       "8        cultural exclusive  \n",
       "9         cultural agnostic  \n",
       "10  cultural representative  \n",
       "11       cultural exclusive  \n",
       "12        cultural agnostic  \n",
       "13       cultural exclusive  \n",
       "14        cultural agnostic  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REMAP = True\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['item'] = test_df['item']\n",
    "results['name'] = test_df['name']\n",
    "results['predictions'] = predictions\n",
    "\n",
    "if REMAP:\n",
    "    remap_dict = {\n",
    "        0: 'cultural exclusive',\n",
    "        1: 'cultural representative',\n",
    "        2: 'cultural agnostic'\n",
    "    }\n",
    "    results['predictions'] = results['predictions'].map(remap_dict)\n",
    "\n",
    "results.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-cR_En9J_A9"
   },
   "outputs": [],
   "source": [
    "results.to_csv(dir_path + \"RoBERTa_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
