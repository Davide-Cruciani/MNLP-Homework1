{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "\n",
    "import joblib\n",
    "import utilsLib as utils\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbc5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "DATASET_LINK = os.environ['DATASET_LINK']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encodings\n",
    "ENCODING_PATH = 'utils'\n",
    "ENCODINGS_NAME = {\n",
    "    \"subcategory\":\"subcategoriesAliases.json\",\n",
    "    \"category\":\"categoriesAliases.json\",\n",
    "    \"label\":\"labelsAliases.json\",\n",
    "    \"type\":\"typesAliases.json\", \n",
    "}\n",
    "\n",
    "# Ensemble Submodules paths\n",
    "MODELS_PATH = \"pretrained_models\"\n",
    "SVC_CATEGORIES_NAME = 'categorySVCWeights.pkl'\n",
    "SVC_DESCRIPTION_NAME = 'DescriptionSVM.pkl'\n",
    "SVM_PARAGRAPH_NAME = 'PenaltySVM_OOV.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = dict()\n",
    "for key in ENCODINGS_NAME:\n",
    "    with open(f'{ENCODING_PATH}/{ENCODINGS_NAME[key]}', 'r') as file:\n",
    "        encodings[key] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bfc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTotal = datasets.load_dataset(DATASET_LINK, token=HF_TOKEN)\n",
    "datasetRaw = datasetTotal['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec3d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseInfoDataset(dataset, encodings, debug=False):\n",
    "    subcategoriesColumn = np.zeros((dataset.shape[0], len(encodings['subcategory'].keys())), dtype=int)\n",
    "    categoriesColumn = np.zeros((dataset.shape[0], len(encodings['category'].keys())), dtype=int)\n",
    "    typesColumn = np.zeros((dataset.shape[0], len(encodings['type'].keys())), dtype=int)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        try:\n",
    "            subcategoriesColumn[i][encodings['subcategory'][dataset['subcategory'].iloc[i]]] = 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            categoriesColumn[i][encodings['category'][dataset['category'].iloc[i]]] = 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            typesColumn[i][encodings['type'][dataset['type'].iloc[i]]] = 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if debug:\n",
    "        print(subcategoriesColumn.shape)\n",
    "        print(categoriesColumn.shape)\n",
    "        print(typesColumn.shape)\n",
    "    \n",
    "    return np.concatenate(\n",
    "        (subcategoriesColumn,categoriesColumn,typesColumn),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1da5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 133)\n"
     ]
    }
   ],
   "source": [
    "baseData = baseInfoDataset(datasetRaw, encodings)\n",
    "print(baseData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137c0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphCache = {'data': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e1f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(paragraphCache['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2384d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphDataset(dataset, trainset, cache=paragraphCache):\n",
    "    UNK_TOKEN = '<UNK>'\n",
    "    target = dataset.copy()\n",
    "    columsToEncode = ['type', 'subcategory']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    processor = utils.TextProcessor('utils/mostInfluentWords.json', \n",
    "                                            removed=['ˈ', 'also', 'since', 'many'],\n",
    "                                            unkonwn=UNK_TOKEN)\n",
    "   \n",
    "    encoder.fit_transform(trainset[columsToEncode])\n",
    "    encodingTS = encoder.transform(target[['type', 'subcategory']]).toarray().astype(float)\n",
    "   \n",
    "    if cache['data'] is None:\n",
    "        paragraphs = utils.getText(target, lang='en', max_workers=10)\n",
    "        cache['data'] = paragraphs\n",
    "    else:\n",
    "        paragraphs = cache['data']\n",
    "    target['paragraph'] = paragraphs\n",
    "   \n",
    "    tfidfColumn = processor.process(target, column='paragraph')\n",
    "    \n",
    "    \n",
    "    \n",
    "    vocab = sorted(set(word for doc in target['paragraph'] for word in processor.tokenize(doc)))\n",
    "    word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    word_index[UNK_TOKEN] = len(word_index)\n",
    "    \n",
    "    encodingTFIDF = np.array([processor.vectorize(doc_tfidf, word_index) for doc_tfidf in tfidfColumn])\n",
    "    \n",
    "    return np.hstack([encodingTFIDF, encodingTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8e0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:00<00:00, 94951.50it/s]\n",
      "100%|██████████| 505/505 [00:00<00:00, 247531.09it/s]\n",
      "100%|██████████| 428/428 [00:00<00:00, 197054.02it/s]\n",
      "100%|██████████| 464/464 [00:00<00:00, 158662.73it/s]\n",
      "100%|██████████| 520/520 [00:00<00:00, 226530.75it/s]\n",
      "100%|██████████| 516/516 [00:00<00:00, 507446.86it/s]\n",
      "100%|██████████| 511/511 [00:00<00:00, 600361.16it/s]\n",
      "100%|██████████| 392/392 [00:00<?, ?it/s]\n",
      "100%|██████████| 440/440 [00:00<00:00, 439927.00it/s]\n",
      "100%|██████████| 413/413 [00:00<?, ?it/s]\n",
      "100%|██████████| 586/586 [00:00<00:00, 160298.84it/s]\n",
      "100%|██████████| 414/414 [00:00<?, ?it/s]\n",
      "100%|██████████| 489/489 [00:00<?, ?it/s]\n",
      "100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "100%|██████████| 435/435 [00:00<?, ?it/s]\n",
      "100%|██████████| 449/449 [00:00<00:00, 444370.57it/s]\n",
      "100%|██████████| 453/453 [00:00<?, ?it/s]\n",
      "100%|██████████| 364/364 [00:00<00:00, 490751.09it/s]\n",
      "100%|██████████| 532/532 [00:00<?, ?it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 300/300 [00:00<00:00, 865.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 5310)\n"
     ]
    }
   ],
   "source": [
    "paragraphData = paragraphDataset(datasetRaw, datasetTotal['train'].to_pandas())\n",
    "print(paragraphData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9276490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptionDataset(dataset, trainset):\n",
    "    UNK_TOKEN = '<UNK>'\n",
    "    target = dataset.copy()\n",
    "    columsToEncode = ['type', 'subcategory']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    processor = utils.TextProcessor(None, removed=['ˈ', 'also', 'since', 'many'],\n",
    "                                    unkonwn=UNK_TOKEN)\n",
    "   \n",
    "    encoder.fit_transform(trainset[columsToEncode])\n",
    "    encodingTS = encoder.transform(target[['type', 'subcategory']]).toarray().astype(float)\n",
    "   \n",
    "    tfidfColumn = processor.process(target, column='description')\n",
    "    \n",
    "    vocab = sorted(set(word for doc in target['description'] for word in processor.tokenize(doc)))\n",
    "    word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    word_index[UNK_TOKEN] = len(word_index)\n",
    "    \n",
    "    encodingTFIDF = np.array([processor.vectorize(doc_tfidf, word_index) for doc_tfidf in tfidfColumn])\n",
    "    \n",
    "    return np.hstack([encodingTFIDF, encodingTS])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79510e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<?, ?it/s]\n",
      "100%|██████████| 46/46 [00:00<?, ?it/s]\n",
      "100%|██████████| 61/61 [00:00<?, ?it/s]\n",
      "100%|██████████| 49/49 [00:00<?, ?it/s]\n",
      "100%|██████████| 104/104 [00:00<00:00, 104281.05it/s]\n",
      "100%|██████████| 44/44 [00:00<?, ?it/s]\n",
      "100%|██████████| 64/64 [00:00<?, ?it/s]\n",
      "100%|██████████| 62/62 [00:00<?, ?it/s]\n",
      "100%|██████████| 72/72 [00:00<?, ?it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 117128.17it/s]\n",
      "100%|██████████| 78/78 [00:00<?, ?it/s]\n",
      "100%|██████████| 61/61 [00:00<?, ?it/s]\n",
      "100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "100%|██████████| 67/67 [00:00<?, ?it/s]\n",
      "100%|██████████| 87/87 [00:00<?, ?it/s]\n",
      "100%|██████████| 61/61 [00:00<?, ?it/s]\n",
      "100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 36927.11it/s]\n",
      "100%|██████████| 86/86 [00:00<?, ?it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 300/300 [00:00<00:00, 5399.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "descriptionData = descriptionDataset(datasetRaw, datasetTotal['train'].to_pandas())\n",
    "print(descriptionData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "165cfcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      0\n",
      "2      2\n",
      "3      0\n",
      "4      2\n",
      "      ..\n",
      "295    1\n",
      "296    2\n",
      "297    0\n",
      "298    2\n",
      "299    0\n",
      "Name: label, Length: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = datasetRaw['label'].apply(lambda x: encodings['label'][x])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbcaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svcBaseInfo = joblib.load(f\"{MODELS_PATH}/{SVC_CATEGORIES_NAME}\")\n",
    "svcDescription = joblib.load(f\"{MODELS_PATH}/{SVC_DESCRIPTION_NAME}\")\n",
    "svcParagraph = joblib.load(f\"{MODELS_PATH}/{SVM_PARAGRAPH_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21f25212",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "This 'SVC' has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_available_if.py:32\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor._check\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     check_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:831\u001b[0m, in \u001b[0;36mBaseSVC._check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability:\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba is not available when probability=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when probability=False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m predictionsBaseInfo \u001b[38;5;241m=\u001b[39m svcBaseInfo\u001b[38;5;241m.\u001b[39mpredict_proba(baseData)\n\u001b[1;32m----> 2\u001b[0m predictionsParagraph \u001b[38;5;241m=\u001b[39m \u001b[43msvcParagraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(paragraphData)\n\u001b[0;32m      3\u001b[0m predictionsDescription \u001b[38;5;241m=\u001b[39m svcDescription\u001b[38;5;241m.\u001b[39mpredict_proba(descriptionData)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_available_if.py:43\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mowner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m         out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;66;03m# This makes it possible to use the decorated method as an unbound method,\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# for instance when monkeypatching.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_available_if.py:34\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor._check\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m     32\u001b[0m     check_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(obj)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_result:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg)\n",
      "\u001b[1;31mAttributeError\u001b[0m: This 'SVC' has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "predictionsBaseInfo = svcBaseInfo.predict_proba(baseData)\n",
    "predictionsParagraph = svcParagraph.predict_proba(paragraphData)\n",
    "predictionsDescription = svcDescription.predict_proba(descriptionData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilitiesFinal = np.stack((predictionsBaseInfo, predictionsParagraph, predictionsDescription), axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
