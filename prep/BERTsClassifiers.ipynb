{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JnmdJsMZK55"
   },
   "source": [
    "# Transformers Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 25638,
     "status": "ok",
     "timestamp": 1745857548759,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "3lRNucX-ZK58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import evaluate\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "from wikidata.client import Client\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, Trainer, TrainingArguments, DataCollatorWithPadding, set_seed, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745857548775,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "n366eQk50yvX"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMFmU7zwpOff"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745857548812,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "f8IDPKdrZK5-"
   },
   "outputs": [],
   "source": [
    "dir_path = \"/mnt/c/Users/fede6/Desktop/HW1/\"\n",
    "train_path = \"train.csv\"\n",
    "dev_path = \"valid.csv\"\n",
    "test_path =  \"test_unlabeled.csv\"\n",
    "\n",
    "train_df = pd.read_csv(dir_path + train_path, encoding='utf-8')\n",
    "dev_df = pd.read_csv(dir_path + dev_path, encoding='utf-8')\n",
    "test_df = pd.read_csv(dir_path + test_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1745857549855,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sfBUCdZNZK5_"
   },
   "outputs": [],
   "source": [
    "def save_txt(filename, path, txt):\n",
    "    with open(path + filename, 'w', encoding='utf-8') as output:\n",
    "        json.dump(txt, output, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_txt(filename, path):\n",
    "    with open(path + filename, 'r', encoding='utf-8') as input_file:\n",
    "        return json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "def extract_qid(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def get_wiki_link(qid, lang='en'):\n",
    "    try:\n",
    "        entity = client.get(qid, load=True)\n",
    "        sitelinks = entity.data.get('sitelinks', {})\n",
    "        page_info = sitelinks.get(f'{lang}wiki')\n",
    "        return page_info['url'] if page_info else None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR retrieving Wikipedia link for {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_paragraphs(wikipedia_link):\n",
    "    try:\n",
    "        response = requests.get(wikipedia_link, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.find('div', class_='mw-content-ltr mw-parser-output')\n",
    "        if not content:\n",
    "            return None\n",
    "\n",
    "        paragraphs = []\n",
    "        for p in content.find_all('p'):\n",
    "            text = p.get_text(separator=\" \", strip=True)\n",
    "            text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR! Link {wikipedia_link}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_item(index, item, df, lang):\n",
    "    try:\n",
    "        qid = extract_qid(item)\n",
    "        link = get_wiki_link(qid, lang)\n",
    "        paragraph = get_paragraphs(link)\n",
    "\n",
    "        if not link:\n",
    "            print(f\"WARNING: missing Wikipedia link for QID {qid}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        if not paragraph:\n",
    "            print(f\"WARNING: empty or missing content for {link}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        return index, paragraph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing item {item} (QID: {qid if 'qid' in locals() else 'UNKNOWN'}): {e}\")\n",
    "        return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "def text_extraction(df, lang='en', max_workers=16):\n",
    "    results = [None] * len(df)\n",
    "    items = list(enumerate(df['item']))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_item, idx, item, df, lang): idx for idx, item in items}\n",
    "\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            idx, paragraph = future.result()\n",
    "            results[idx] = paragraph\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACTED:\n",
    "    train_txt = load_txt(filename=\"train_txts.txt\", path=dir_path)\n",
    "    valid_txt = load_txt(filename=\"dev_txts.txt\",   path=dir_path)\n",
    "    test_txt = load_txt(filename=\"test_txts.txt\",   path=dir_path)\n",
    "else:\n",
    "    train_txt = text_extraction(df=train_df)\n",
    "    valid_txt = text_extraction(df=dev_df)\n",
    "    test_txt  = text_extraction(df=test_df)\n",
    "\n",
    "train_df['paragraph'] = train_txt\n",
    "dev_df['paragraph'] = valid_txt\n",
    "test_df['paragraph'] = test_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q32786</td>\n",
       "      <td>916</td>\n",
       "      <td>2012 film by M. Mohanan</td>\n",
       "      <td>entity</td>\n",
       "      <td>films</td>\n",
       "      <td>film</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>916 is a 2012 Indian Malayalam -language drama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q371</td>\n",
       "      <td>!!!</td>\n",
       "      <td>American dance-punk band from California</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>musical group</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>!!! ( / tʃ ( ɪ ) k . tʃ ( ɪ ) k . tʃ ( ɪ ) k /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3729947</td>\n",
       "      <td>¡Soborno!</td>\n",
       "      <td>Mort &amp; Phil comic</td>\n",
       "      <td>entity</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>comics</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>¡Soborno! (English: Bribery! ) is a 1977 comic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q158611</td>\n",
       "      <td>+44</td>\n",
       "      <td>American band</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>musical group</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>+44 (read as Plus Forty-four ) was an American...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q280375</td>\n",
       "      <td>1 Monk Street</td>\n",
       "      <td>building in Monmouth, Wales</td>\n",
       "      <td>entity</td>\n",
       "      <td>architecture</td>\n",
       "      <td>building</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>1 Monk Street, Monmouth was built as a Working...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      item           name  \\\n",
       "0    http://www.wikidata.org/entity/Q32786            916   \n",
       "1      http://www.wikidata.org/entity/Q371            !!!   \n",
       "2  http://www.wikidata.org/entity/Q3729947      ¡Soborno!   \n",
       "3   http://www.wikidata.org/entity/Q158611            +44   \n",
       "4   http://www.wikidata.org/entity/Q280375  1 Monk Street   \n",
       "\n",
       "                                description    type          category  \\\n",
       "0                   2012 film by M. Mohanan  entity             films   \n",
       "1  American dance-punk band from California  entity             music   \n",
       "2                         Mort & Phil comic  entity  comics and anime   \n",
       "3                             American band  entity             music   \n",
       "4               building in Monmouth, Wales  entity      architecture   \n",
       "\n",
       "     subcategory                    label  \\\n",
       "0           film       cultural exclusive   \n",
       "1  musical group  cultural representative   \n",
       "2         comics  cultural representative   \n",
       "3  musical group  cultural representative   \n",
       "4       building       cultural exclusive   \n",
       "\n",
       "                                           paragraph  \n",
       "0  916 is a 2012 Indian Malayalam -language drama...  \n",
       "1  !!! ( / tʃ ( ɪ ) k . tʃ ( ɪ ) k . tʃ ( ɪ ) k /...  \n",
       "2  ¡Soborno! (English: Bribery! ) is a 1977 comic...  \n",
       "3  +44 (read as Plus Forty-four ) was an American...  \n",
       "4  1 Monk Street, Monmouth was built as a Working...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attention**!\n",
    "In order to deliver the predicitons on the unlabeled test_set. In order to evaluate is sufficient to do set **isTest=True**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745857549864,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "nAzfHZZlpOfh"
   },
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    'cultural agnostic':       2,\n",
    "    'cultural representative': 1,\n",
    "    'cultural exclusive':      0\n",
    "}\n",
    "\n",
    "class PLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, df, isTest=False):\n",
    "        self.isTest = isTest\n",
    "        self.encodings = encodings\n",
    "        self.size = len(df['item'].to_list())\n",
    "        if self.isTest == False:\n",
    "            self.labels = [mapper[label] for label in df['label']]\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]).to(device) for k, v in self.encodings.items()}\n",
    "        \n",
    "        if self.isTest:\n",
    "            return item\n",
    "        else:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)            \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOK9uFuqqiVt"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745857549868,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "zaAJp8vzqkS_"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_9p48HNroW-"
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549880,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sNU3_3WLrrF_"
   },
   "outputs": [],
   "source": [
    "def model_init(model_name, n_classes=3, padding=True, truncation=True):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True, output_attentions=False, output_hidden_states=False, num_labels=n_classes).to(device)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "  return model, tokenizer, data_collator\n",
    "\n",
    "def tokenization(df, tokenizer):\n",
    "    return tokenizer(df[\"paragraph\"].to_list(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 391\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFl1YZoypOfi"
   },
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549882,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "lL3hansiZK5_"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549884,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "Eg1_fGFJpOfj"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39653,
     "status": "ok",
     "timestamp": 1745857589550,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "uiYDuTRNZK6A",
    "outputId": "413439de-3a91-4abf-9d55-3f21da573380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, isTest=False)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, isTest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745857589567,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "fS2h8HDwZK6B"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 961697,
     "status": "ok",
     "timestamp": 1745858551266,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "_pPwmKC6ZK6C",
    "outputId": "274152db-76a2-45d2-a69c-9928230e710d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2737' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2737/3910 17:42 < 07:35, 2.57 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.881600</td>\n",
       "      <td>0.620792</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.705515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.559818</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.741703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.534733</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.752723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.600848</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.748466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.636548</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.768821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.752649</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.745714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.817434</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.740811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2737, training_loss=0.481494403207742, metrics={'train_runtime': 1063.3355, 'train_samples_per_second': 29.393, 'train_steps_per_second': 3.677, 'total_flos': 2898570840966144.0, 'train_loss': 0.481494403207742, 'epoch': 3.5})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 5318,
     "status": "ok",
     "timestamp": 1745858556581,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "bSRgVCQnZK6C",
    "outputId": "d37329ba-e4e5-4bcd-c93d-f3eb9029d3a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5347334742546082,\n",
       " 'eval_accuracy': 0.7666666666666667,\n",
       " 'eval_f1': 0.7527233115468409,\n",
       " 'eval_runtime': 6.7134,\n",
       " 'eval_samples_per_second': 44.687,\n",
       " 'eval_steps_per_second': 5.66,\n",
       " 'epoch': 3.5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taN8euqVpOfl"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745858556589,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "_yJNbgDEpOfm"
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-hTTqaY_IIU",
    "outputId": "1f8e20aa-cc8f-411d-cf6a-58aca8a7344c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, isTest=False)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, isTest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "z0m7JGXw_IIV"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NferTjq9pOfm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/3910 29:04 < 19:23, 1.34 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.873700</td>\n",
       "      <td>0.654126</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.631042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.747070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.615110</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.737141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.740719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.705668</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.750575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.793203</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.740052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.49736401166858984, metrics={'train_runtime': 1744.8959, 'train_samples_per_second': 17.912, 'train_steps_per_second': 2.241, 'total_flos': 4934165922653184.0, 'train_loss': 0.49736401166858984, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ixz2xk-kpOfn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5664123296737671,\n",
       " 'eval_accuracy': 0.7633333333333333,\n",
       " 'eval_f1': 0.747069841162474,\n",
       " 'eval_runtime': 10.7632,\n",
       " 'eval_samples_per_second': 27.873,\n",
       " 'eval_steps_per_second': 3.531,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14gduCy_pOfn"
   },
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uJ18vZgBCmPR"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "J3L3ZRA7pOfn"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zTYy3v98Cd7R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, isTest=False)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, isTest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yWC_S5KnCd7R"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_AInJCUkpOfo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/3910 29:25 < 19:37, 1.33 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.602486</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.721386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.575793</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.794663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.578462</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.768546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.618061</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.776604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.731010</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.771274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.866831</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.740185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2346, training_loss=0.5111339468277121, metrics={'train_runtime': 1766.1796, 'train_samples_per_second': 17.696, 'train_steps_per_second': 2.214, 'total_flos': 4934165922653184.0, 'train_loss': 0.5111339468277121, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KRRJtDRhpOfo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5757932066917419,\n",
       " 'eval_accuracy': 0.8033333333333333,\n",
       " 'eval_f1': 0.7946633866399488,\n",
       " 'eval_runtime': 9.9552,\n",
       " 'eval_samples_per_second': 30.135,\n",
       " 'eval_steps_per_second': 3.817,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After different trials, the RoBERTa-base model showed better performance with respect to the DistilBERT and BERT-base models.  \n",
    "All the models showed that after 1 epoch the train loss and validation loss start to diverge. In order to show this behavior, the models has been trained for at most 5 epochs and the early stopping callback has been used to interrupt the training when the validation loss increase for 4 *eval_steps*.  \n",
    "In this section, the predictions of first elements of the test set made by the last training experiment of the RoBERTa-base, while in the next section [**Best Model**], instead, the best trained RoBERTa-base model predictions are showed and saved as file csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PLMDataset(tokenization(test_df, tokenizer=tokenizer), df=test_df, isTest=True)\n",
    "\n",
    "preds_struct = trainer.predict(test_dataset=test_dataset)\n",
    "predictions = np.argmax(preds_struct.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2427430</td>\n",
       "      <td>Northeast Flag Replacement</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q125482</td>\n",
       "      <td>imam</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15789</td>\n",
       "      <td>FC Bayern Munich</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q582496</td>\n",
       "      <td>Fome Zero</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q572811</td>\n",
       "      <td>Anthony Award</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1866547</td>\n",
       "      <td>Livraria Bertrand</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.wikidata.org/entity/Q19081</td>\n",
       "      <td>prokaryotes</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.wikidata.org/entity/Q474090</td>\n",
       "      <td>narrative poetry</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1266300</td>\n",
       "      <td>Neue Slowenische Kunst</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.wikidata.org/entity/Q193654</td>\n",
       "      <td>short-track speed skating</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://www.wikidata.org/entity/Q372</td>\n",
       "      <td>We Live in Public</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://www.wikidata.org/entity/Q820887</td>\n",
       "      <td>University of Florence</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.wikidata.org/entity/Q503269</td>\n",
       "      <td>seamount</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.wikidata.org/entity/Q57318</td>\n",
       "      <td>free imperial city</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://www.wikidata.org/entity/Q25239</td>\n",
       "      <td>Alnus</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       item                        name  \\\n",
       "0   http://www.wikidata.org/entity/Q2427430  Northeast Flag Replacement   \n",
       "1    http://www.wikidata.org/entity/Q125482                        imam   \n",
       "2     http://www.wikidata.org/entity/Q15789            FC Bayern Munich   \n",
       "3    http://www.wikidata.org/entity/Q582496                   Fome Zero   \n",
       "4    http://www.wikidata.org/entity/Q572811               Anthony Award   \n",
       "5   http://www.wikidata.org/entity/Q1866547           Livraria Bertrand   \n",
       "6     http://www.wikidata.org/entity/Q19081                 prokaryotes   \n",
       "7    http://www.wikidata.org/entity/Q474090            narrative poetry   \n",
       "8   http://www.wikidata.org/entity/Q1266300      Neue Slowenische Kunst   \n",
       "9    http://www.wikidata.org/entity/Q193654   short-track speed skating   \n",
       "10      http://www.wikidata.org/entity/Q372           We Live in Public   \n",
       "11   http://www.wikidata.org/entity/Q820887      University of Florence   \n",
       "12   http://www.wikidata.org/entity/Q503269                    seamount   \n",
       "13    http://www.wikidata.org/entity/Q57318          free imperial city   \n",
       "14    http://www.wikidata.org/entity/Q25239                       Alnus   \n",
       "\n",
       "                predictions  \n",
       "0        cultural exclusive  \n",
       "1   cultural representative  \n",
       "2   cultural representative  \n",
       "3        cultural exclusive  \n",
       "4        cultural exclusive  \n",
       "5        cultural exclusive  \n",
       "6         cultural agnostic  \n",
       "7         cultural agnostic  \n",
       "8        cultural exclusive  \n",
       "9         cultural agnostic  \n",
       "10  cultural representative  \n",
       "11       cultural exclusive  \n",
       "12        cultural agnostic  \n",
       "13  cultural representative  \n",
       "14        cultural agnostic  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['item'] = test_df['item']\n",
    "results['name'] = test_df['name']\n",
    "results['predictions'] = predictions\n",
    "\n",
    "remap_dict = {\n",
    "    0: 'cultural exclusive',\n",
    "    1: 'cultural representative',\n",
    "    2: 'cultural agnostic'\n",
    "}\n",
    "\n",
    "results['predictions'] = results['predictions'].map(remap_dict)\n",
    "results.to_csv(dir_path + \"RoBERTa_predictions.csv\")\n",
    "\n",
    "results.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several experiments, observing the metrics on the development set, the best model is **RoBERTa-base model**. Here is presented the evaluation metrics on the validation set and then the predictions on the first elements of the test set. The best model's predictions are saved in file csv, delivered as final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = dir_path + \"BestRes/results/checkpoint-782\"\n",
    "\n",
    "model, tokenizer, data_coll = model_init(best_model_path)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, df=train_df, isTest=False)\n",
    "val_dataset = PLMDataset(tokenized_devset, df=dev_df, isTest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa base score:\n",
      "Accuracy: 0.8100\n",
      "F1 score: 0.8007\n"
     ]
    }
   ],
   "source": [
    "accuracy = results['eval_accuracy']\n",
    "f1_score = results['eval_f1']\n",
    "\n",
    "print(f\"RoBERTa base score:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PLMDataset(tokenization(test_df, tokenizer=tokenizer), df=test_df, isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_struct = trainer.predict(test_dataset=test_dataset)\n",
    "predictions = np.argmax(preds_struct.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels of the predictions have been mapped in the corresponding cultural class in order to provide a easier readable table. This mapping can be easily removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2427430</td>\n",
       "      <td>Northeast Flag Replacement</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q125482</td>\n",
       "      <td>imam</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15789</td>\n",
       "      <td>FC Bayern Munich</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q582496</td>\n",
       "      <td>Fome Zero</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q572811</td>\n",
       "      <td>Anthony Award</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1866547</td>\n",
       "      <td>Livraria Bertrand</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.wikidata.org/entity/Q19081</td>\n",
       "      <td>prokaryotes</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.wikidata.org/entity/Q474090</td>\n",
       "      <td>narrative poetry</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1266300</td>\n",
       "      <td>Neue Slowenische Kunst</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.wikidata.org/entity/Q193654</td>\n",
       "      <td>short-track speed skating</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://www.wikidata.org/entity/Q372</td>\n",
       "      <td>We Live in Public</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://www.wikidata.org/entity/Q820887</td>\n",
       "      <td>University of Florence</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.wikidata.org/entity/Q503269</td>\n",
       "      <td>seamount</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.wikidata.org/entity/Q57318</td>\n",
       "      <td>free imperial city</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://www.wikidata.org/entity/Q25239</td>\n",
       "      <td>Alnus</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       item                        name  \\\n",
       "0   http://www.wikidata.org/entity/Q2427430  Northeast Flag Replacement   \n",
       "1    http://www.wikidata.org/entity/Q125482                        imam   \n",
       "2     http://www.wikidata.org/entity/Q15789            FC Bayern Munich   \n",
       "3    http://www.wikidata.org/entity/Q582496                   Fome Zero   \n",
       "4    http://www.wikidata.org/entity/Q572811               Anthony Award   \n",
       "5   http://www.wikidata.org/entity/Q1866547           Livraria Bertrand   \n",
       "6     http://www.wikidata.org/entity/Q19081                 prokaryotes   \n",
       "7    http://www.wikidata.org/entity/Q474090            narrative poetry   \n",
       "8   http://www.wikidata.org/entity/Q1266300      Neue Slowenische Kunst   \n",
       "9    http://www.wikidata.org/entity/Q193654   short-track speed skating   \n",
       "10      http://www.wikidata.org/entity/Q372           We Live in Public   \n",
       "11   http://www.wikidata.org/entity/Q820887      University of Florence   \n",
       "12   http://www.wikidata.org/entity/Q503269                    seamount   \n",
       "13    http://www.wikidata.org/entity/Q57318          free imperial city   \n",
       "14    http://www.wikidata.org/entity/Q25239                       Alnus   \n",
       "\n",
       "                predictions  \n",
       "0        cultural exclusive  \n",
       "1   cultural representative  \n",
       "2   cultural representative  \n",
       "3        cultural exclusive  \n",
       "4        cultural exclusive  \n",
       "5        cultural exclusive  \n",
       "6         cultural agnostic  \n",
       "7         cultural agnostic  \n",
       "8        cultural exclusive  \n",
       "9         cultural agnostic  \n",
       "10  cultural representative  \n",
       "11       cultural exclusive  \n",
       "12        cultural agnostic  \n",
       "13       cultural exclusive  \n",
       "14        cultural agnostic  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['item'] = test_df['item']\n",
    "results['name'] = test_df['name']\n",
    "results['predictions'] = predictions\n",
    "\n",
    "remap_dict = {\n",
    "    0: 'cultural exclusive',\n",
    "    1: 'cultural representative',\n",
    "    2: 'cultural agnostic'\n",
    "}\n",
    "results['predictions'] = results['predictions'].map(remap_dict)\n",
    "\n",
    "results.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(dir_path + \"RoBERTa_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
