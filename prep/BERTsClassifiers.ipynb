{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JnmdJsMZK55"
   },
   "source": [
    "# Transformers Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 25638,
     "status": "ok",
     "timestamp": 1745857548759,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "3lRNucX-ZK58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epicmusk/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, Trainer, TrainingArguments, DataCollatorWithPadding, set_seed, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745857548775,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "n366eQk50yvX"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMFmU7zwpOff"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745857548812,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "f8IDPKdrZK5-"
   },
   "outputs": [],
   "source": [
    "dir_path = \"/mnt/c/Users/fede6/Desktop/HW1/\"\n",
    "train_path = \"train.csv\"\n",
    "dev_path = \"valid.csv\"\n",
    "\n",
    "train_df = pd.read_csv(dir_path + train_path, encoding='utf-8')\n",
    "dev_df = pd.read_csv(dir_path + dev_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1745857549855,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sfBUCdZNZK5_"
   },
   "outputs": [],
   "source": [
    "def save_txt(filename, path, txt):\n",
    "    with open(path + filename, 'w', encoding='utf-8') as output:\n",
    "        json.dump(txt, output, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_txt(filename, path):\n",
    "    with open(path + filename, 'r', encoding='utf-8') as input_file:\n",
    "        return json.load(input_file)\n",
    "\n",
    "train_txt = load_txt(filename=\"train_txts.txt\", path=dir_path)\n",
    "valid_txt = load_txt(filename=\"dev_txts.txt\",   path=dir_path)\n",
    "\n",
    "train_df['paragraph'] = train_txt\n",
    "dev_df['paragraph'] = valid_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745857549864,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "nAzfHZZlpOfh"
   },
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    'cultural agnostic':       2,\n",
    "    'cultural representative': 1,\n",
    "    'cultural exclusive':      0\n",
    "}\n",
    "\n",
    "class PLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = [mapper[label] for label in labels]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]).to(device) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOK9uFuqqiVt"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745857549868,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "zaAJp8vzqkS_"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_9p48HNroW-"
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549880,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "sNU3_3WLrrF_"
   },
   "outputs": [],
   "source": [
    "def model_init(model_name, n_classes=3, padding=True, truncation=True):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True, output_attentions=False, output_hidden_states=False, num_labels=n_classes).to(device)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "  return model, tokenizer, data_collator\n",
    "\n",
    "def tokenization(df, tokenizer):\n",
    "    return tokenizer(df[\"paragraph\"].to_list(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFl1YZoypOfi"
   },
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549882,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "lL3hansiZK5_"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745857549884,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "Eg1_fGFJpOfj"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39653,
     "status": "ok",
     "timestamp": 1745857589550,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "uiYDuTRNZK6A",
    "outputId": "413439de-3a91-4abf-9d55-3f21da573380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, train_df['label'])\n",
    "val_dataset = PLMDataset(tokenized_devset, dev_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745857589551,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "iqXu7g6Z1p8n"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 391\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745857589567,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "fS2h8HDwZK6B"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 961697,
     "status": "ok",
     "timestamp": 1745858551266,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "_pPwmKC6ZK6C",
    "outputId": "274152db-76a2-45d2-a69c-9928230e710d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2737' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2737/3910 16:48 < 07:12, 2.71 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.709178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.541131</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.755749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.532408</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.785511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.599164</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.740003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>0.705475</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.753761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.796741</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.732461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.878940</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.739275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|█████████████████████████████████████████| 4.20k/4.20k [00:00<00:00, 4.58MB/s]\n",
      "Downloading builder script: 100%|█████████████████████████████████████████| 6.79k/6.79k [00:00<00:00, 15.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2737, training_loss=0.4773688464431289, metrics={'train_runtime': 1009.3288, 'train_samples_per_second': 30.966, 'train_steps_per_second': 3.874, 'total_flos': 2898570840966144.0, 'train_loss': 0.4773688464431289, 'epoch': 3.5})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 5318,
     "status": "ok",
     "timestamp": 1745858556581,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "bSRgVCQnZK6C",
    "outputId": "d37329ba-e4e5-4bcd-c93d-f3eb9029d3a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5324076414108276,\n",
       " 'eval_accuracy': 0.7966666666666666,\n",
       " 'eval_f1': 0.7855108834189486,\n",
       " 'eval_runtime': 6.1237,\n",
       " 'eval_samples_per_second': 48.99,\n",
       " 'eval_steps_per_second': 6.205,\n",
       " 'epoch': 3.5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taN8euqVpOfl"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745858556589,
     "user": {
      "displayName": "federico miscione",
      "userId": "14163495743086838850"
     },
     "user_tz": -120
    },
    "id": "_yJNbgDEpOfm"
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-hTTqaY_IIU",
    "outputId": "1f8e20aa-cc8f-411d-cf6a-58aca8a7344c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, train_df['label'])\n",
    "val_dataset = PLMDataset(tokenized_devset, dev_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z0m7JGXw_IIV"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NferTjq9pOfm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2737' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2737/3910 32:25 < 13:54, 1.41 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>0.640640</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.696475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.553624</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.761861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.552142</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.753139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.628857</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.736799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.673191</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.776473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2346</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.788850</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.739483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2737</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.905424</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.761111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2737, training_loss=0.4573457471898406, metrics={'train_runtime': 1946.8589, 'train_samples_per_second': 16.054, 'train_steps_per_second': 2.008, 'total_flos': 5757184693306368.0, 'train_loss': 0.4573457471898406, 'epoch': 3.5})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ixz2xk-kpOfn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5521418452262878,\n",
       " 'eval_accuracy': 0.7666666666666667,\n",
       " 'eval_f1': 0.7531392880965376,\n",
       " 'eval_runtime': 9.9522,\n",
       " 'eval_samples_per_second': 30.144,\n",
       " 'eval_steps_per_second': 3.818,\n",
       " 'epoch': 3.5}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14gduCy_pOfn"
   },
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uJ18vZgBCmPR"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "J3L3ZRA7pOfn"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zTYy3v98Cd7R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, data_coll = model_init(model_name)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, train_df['label'])\n",
    "val_dataset = PLMDataset(tokenized_devset, dev_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yWC_S5KnCd7R"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    output_dir=dir_path + model_name + \"_res/results\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AInJCUkpOfo"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "KRRJtDRhpOfo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5547690987586975,\n",
       " 'eval_model_preparation_time': 0.0066,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_f1': 0.8007156768624658,\n",
       " 'eval_runtime': 10.1975,\n",
       " 'eval_samples_per_second': 29.419,\n",
       " 'eval_steps_per_second': 3.726}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several experiments, observing the metrics on the development set, the best model is **RoBERTa base model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = dir_path + \"BestRes/results/checkpoint-782\"\n",
    "\n",
    "model, tokenizer, data_coll = model_init(best_model_path)\n",
    "\n",
    "tokenized_trainset = tokenization(train_df, tokenizer=tokenizer)\n",
    "tokenized_devset =   tokenization(dev_df, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = PLMDataset(tokenized_trainset, train_df['label'])\n",
    "val_dataset = PLMDataset(tokenized_devset, dev_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    save_only_model=True,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    greater_is_better=False,\n",
    "    eval_steps=WARMUP_STEPS,\n",
    "    save_steps=WARMUP_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=WARMUP_STEPS,\n",
    "    logging_dir=dir_path+\"logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    per_device_train_batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_coll,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa base score:\n",
      "Accuracy: 0.8100\n",
      "F1 score: 0.8007\n"
     ]
    }
   ],
   "source": [
    "accuracy = results['eval_accuracy']\n",
    "f1_score = results['eval_f1']\n",
    "\n",
    "print(f\"RoBERTa base score:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 score: {f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
