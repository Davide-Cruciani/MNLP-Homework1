{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import che stavano già nel notebook\n",
    "import re\n",
    "import nltk\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "import spacy\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from wikidata.client import Client\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c959dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def extract_qid(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def get_wiki_link(qid, lang='en'):\n",
    "    try:\n",
    "        entity = client.get(qid, load=True)\n",
    "        sitelinks = entity.data.get('sitelinks', {})\n",
    "        page_info = sitelinks.get(f'{lang}wiki')\n",
    "        return page_info['url'] if page_info else None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR retrieving Wikipedia link for {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_intro_paragraph(wikipedia_link, min_chars=200):\n",
    "    try:\n",
    "        response = requests.get(wikipedia_link, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        if response.is_redirect:\n",
    "            print(f\"WARNING: Redirecting... {response.status_code}\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        content = soup.find('div', class_='mw-content-ltr mw-parser-output')\n",
    "        if not content:\n",
    "            return \"\"\n",
    "\n",
    "        paragraph_text = \"\"\n",
    "        for p in content.find_all('p'):\n",
    "            text = p.get_text(separator=\" \", strip=True)\n",
    "            text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "            paragraph_text += \" \" + text\n",
    "            if len(paragraph_text) > min_chars:\n",
    "                break\n",
    "\n",
    "        return paragraph_text.strip()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ERROR fetching paragraph from {wikipedia_link}: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR parsing content from {wikipedia_link}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def process_item(index, item, df, lang):\n",
    "    try:\n",
    "        qid = extract_qid(item)\n",
    "        link = get_wiki_link(qid, lang)\n",
    "        paragraph = get_intro_paragraph(link)\n",
    "\n",
    "        if not link:\n",
    "            print(f\"WARNING: missing Wikipedia link for QID {qid}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        if not paragraph:\n",
    "            print(f\"WARNING: empty or missing content for {link}\")\n",
    "            return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "        return index, paragraph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing item {item} (QID: {qid if 'qid' in locals() else 'UNKNOWN'}): {e}\")\n",
    "        return index, df['description'][df['item'] == item].values[0]\n",
    "\n",
    "def text_extraction(df, lang='en', max_workers=10):\n",
    "    results = [None] * len(df)\n",
    "    items = list(enumerate(df['item']))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_item, idx, item, df, lang): idx for idx, item in items}\n",
    "\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            idx, paragraph = future.result()\n",
    "            results[idx] = paragraph\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio utilizzo\n",
    "train_txt = text_extraction(df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo effettuato dopo l'addestramento del modello per mappare le 50 parole più \"influenti\" nella classificazione da parte del modello\n",
    "\n",
    "tfidf_size = len(word_index)\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\Class: {class_name}\")\n",
    "    top_features = np.argsort(best_model.coef_[i])[::-1]\n",
    "\n",
    "    count = 0\n",
    "    for feat_idx in top_features:\n",
    "        if feat_idx < tfidf_size:\n",
    "            word = reverse_word_index[feat_idx]\n",
    "            weight = best_model.coef_[i][feat_idx]\n",
    "            print(f\"{word}: {weight:.4f}\")\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
